// ğŸ“ index.js - ì˜¤ë””í‹° ìš´ì˜ ë§¤ë‰´ì–¼ ì˜¤í”ˆë¶ ì±—ë´‡ (Slack + ë¬¸ì„œê²€ìƒ‰ + ë¡œê·¸ ì €ì¥)
require('dotenv').config();
const fs = require('fs');
const path = require('path');
const { App } = require('@slack/bolt');
const { ChatOpenAI } = require('langchain/chat_models/openai');
const { RecursiveCharacterTextSplitter } = require('langchain/text_splitter');
const { OpenAIEmbeddings } = require('langchain/embeddings/openai');
const { MemoryVectorStore } = require('langchain/vectorstores/memory');
const { RetrievalQAChain } = require('langchain/chains');
const { PDFLoader } = require('langchain/document_loaders/fs/pdf');
const { TextLoader } = require('langchain/document_loaders/fs/text');
const { DocxLoader } = require('langchain/document_loaders/fs/docx');

const app = new App({
  token: process.env.SLACK_BOT_TOKEN,
  signingSecret: process.env.SLACK_SIGNING_SECRET,
});

const LOG_PATH = path.join(__dirname, 'logs');
if (!fs.existsSync(LOG_PATH)) fs.mkdirSync(LOG_PATH);

// âœ… Slack "challenge" URL ê²€ì¦ ëŒ€ì‘ (Renderìš©)
app.receiver.app.post('/', (req, res) => {
  if (req.body.type === 'url_verification') {
    return res.status(200).send(req.body.challenge);
  }
  return res.status(200).send('OK');
});

async function loadAllDocs() {
  const dirPath = path.join(__dirname, 'docs');
  const files = fs.readdirSync(dirPath);
  const loaders = [];

  for (const file of files) {
    const ext = path.extname(file).toLowerCase();
    const fullPath = path.join(dirPath, file);
    if (ext === '.pdf') loaders.push(new PDFLoader(fullPath));
    else if (ext === '.txt') loaders.push(new TextLoader(fullPath));
    else if (ext === '.docx') loaders.push(new DocxLoader(fullPath));
  }

  const docs = [];
  for (const loader of loaders) {
    const loaded = await loader.load();
    docs.push(...loaded);
  }

  const splitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000, chunkOverlap: 200 });
  return await splitter.splitDocuments(docs);
}

async function initBot() {
  const splitDocs = await loadAllDocs();
  const vectorStore = await MemoryVectorStore.fromDocuments(splitDocs, new OpenAIEmbeddings());
  const retriever = vectorStore.asRetriever();
  const model = new ChatOpenAI({ modelName: 'gpt-4o', temperature: 0 });
  const qaChain = RetrievalQAChain.fromLLM(model, retriever, { returnSourceDocuments: true });

  async function handleQuestion(text, say, source = "event") {
    try {
      const response = await qaChain.call({ query: text });
      const reply = `ğŸ“˜ *ìš´ì˜ ë§¤ë‰´ì–¼ ë‹µë³€:*\n${response.text}`;
      await say(reply);

      const today = new Date().toISOString().split('T')[0];
      const logFile = path.join(LOG_PATH, `chatlog-${today}.txt`);
      const logEntry = `\n[${new Date().toLocaleString()}] (${source})\nQ: ${text}\nA: ${response.text}\n`;
      fs.appendFileSync(logFile, logEntry);
    } catch (err) {
      await say("âš ï¸ ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.");
      console.error("âŒ GPT ì‘ë‹µ ì—ëŸ¬:", err);
    }
  }

  // âœ… ì¼ë°˜ ë©”ì‹œì§€ (DM í¬í•¨)
  app.message(async ({ message, say }) => {
    if (!message.text || message.subtype === 'bot_message') return;
    await handleQuestion(message.text, say, "message");
  });

  // âœ… ë©˜ì…˜ (@odditybot) ëŒ€ì‘
  app.event("app_mention", async ({ event, say }) => {
    const userQuestion = event.text.replace(/<@[^>]+>\s*/g, "").trim();
    await handleQuestion(userQuestion, say, "mention");
  });

  const PORT = process.env.PORT || 3000;
  await app.start(PORT);
  console.log(`âœ… ì˜¤ë””í‹° ìŠ¬ë™ë´‡ì´ í¬íŠ¸ ${PORT}ì—ì„œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤`);
}

initBot();
